# LLM Load Test Configuration
# Copy this file to .env and adjust the values for your setup

# API Type: ollama, vllm, lmstudio, llamacpp, openai
API_TYPE=ollama

# API Base URL (without trailing slash)
# Examples for different backends:

# Ollama (default)
API_BASE_URL=http://127.0.0.1:11434

# vLLM
# API_BASE_URL=http://127.0.0.1:8000

# LM Studio
# API_BASE_URL=http://127.0.0.1:1234

# llama.cpp server
# API_BASE_URL=http://127.0.0.1:8080

# OpenAI or other OpenAI-compatible APIs
# API_BASE_URL=https://api.openai.com
# API_KEY=your-api-key-here

# API Key (optional, only needed for some backends like OpenAI)
# API_KEY=

# Note: You can override these settings with command line arguments:
# --api-type, --host, --api-key
